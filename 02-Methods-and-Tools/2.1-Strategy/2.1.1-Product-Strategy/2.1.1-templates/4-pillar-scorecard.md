# Pillar Scorecard

### Scoring Framework Deep Dive

#### Expected Impact Scoring (1-5)

**Evaluation Criteria:**
- **User Base Size:** How many users are affected?
- **Frequency:** How often do users encounter this problem/opportunity?
- **Pain/Value Intensity:** How severe is the problem or valuable is the solution?

| Score | User Base | Frequency | Pain/Value Level | Example |
| --- | --- | --- | --- | --- |
| **5** | Most users (70%+) | Daily/Multiple times per day | High pain or high value creation | Core workflow bottleneck affecting all users |
| **4** | Many users (40-70%) | Weekly/Several times per week | Moderate-high pain or value | Important feature that's hard to use |
| **3** | Some users (20-40%) | Monthly/Several times per month | Moderate pain or value | Nice-to-have improvement |
| **2** | Few users (5-20%) | Occasionally/Few times per month | Low-moderate pain or value | Edge case improvement |
| **1** | Very few users (<5%) | Rarely/Few times per year | Low pain or value | Minor inconvenience |

**Example: Discovery & Findability Pillar**
- User Base: 85% of users struggle to find advanced features (High)
- Frequency: Users search for features multiple times per session (High)  
- Pain Level: High - users can't access value they're paying for (High)
- **Score: 5**

#### Certainty of Impact Scoring (1-5)

**Evidence Types & Weighting:**
- **User Research:** High weight (qualitative insights)
- **Usage Data:** High weight (behavioral evidence)
- **Customer Feedback:** Medium weight (direct user input)
- **Support Tickets:** Medium weight (pain point validation)  
- **Industry Trends:** Low weight (contextual support)

| Score | Evidence Strength | Example Evidence Combination |
| --- | --- | --- |
| **5** | Multiple strong sources align | User research + usage data + customer feedback all point to same problem |
| **4** | 2+ strong sources + supporting evidence | Usage data shows drop-off + user interviews confirm friction |
| **3** | 1-2 strong sources | Clear usage data OR comprehensive user research |
| **2** | Limited but directional evidence | Some customer feedback + industry trend support |
| **1** | Mostly speculation | Hypothesis based on limited anecdotal evidence |

**Example: Onboarding & Activation Pillar**
- Usage Data: 45% drop-off at step 3 of onboarding (Strong)
- User Research: 8/10 interviews mentioned onboarding confusion (Strong)
- Support Tickets: 30% of new user tickets about setup (Medium)
- **Score: 4**

#### Clarity of Levers Scoring (1-5)

**Evaluation Questions:**
- How clear are the solution approaches?
- Do we understand what actions would improve this area?
- Can we articulate a theory of change?

| Score | Solution Clarity | Team Confidence | Example |
| --- | --- | --- | --- |
| **5** | Very clear path forward | Team consensus on approach | "Add search + improve navigation + feature highlights" |
| **4** | Clear direction with options | General agreement, some debate on tactics | "Simplify onboarding but unclear if guided tour vs. progressive disclosure" |
| **3** | Directional but needs exploration | Mixed opinions, need experimentation | "Improve performance but unclear if technical vs. UX solutions" |
| **2** | Vague direction | Low confidence in approaches | "Make product more engaging" (too abstract) |
| **1** | No clear solution path | Complete uncertainty | Problem is clear but solutions are mystery |

**Example: Platform & Extensibility Pillar**
- Solution Direction: API improvements + integration marketplace + developer tools
- Team Confidence: Engineering lead confident in technical approach, design lead has UX concept
- **Score: 4**

#### Uniqueness of Levers Scoring (1-5) â­ **Critical 4th Dimension**

**Advantage Types:**
- **Technical:** Proprietary technology, data, or infrastructure
- **Brand:** Market position, trust, or reputation  
- **Network:** User base, partnerships, or ecosystem
- **Talent:** Unique skills or domain expertise
- **Process:** Operational advantages or efficiencies

| Score | Competitive Advantage | Differentiation Potential | Example |
| --- | --- | --- | --- |
| **5** | Strong structural advantages | Competitors can't easily replicate | Unique data set + brand trust in vertical |
| **4** | Clear advantages | Difficult for competitors to match | Strong design team + user research capability |
| **3** | Some advantages | Can differentiate through execution | Good engineering team but others could catch up |
| **2** | Minor advantages | Mostly competing on level field | Slight head start but no lasting advantage |
| **1** | No clear advantages | Playing catch-up to competitors | Trying to match what others do better |

**Example: Discovery & Findability Pillar**
- Technical Advantage: None - search is commoditized
- Brand Advantage: Known for simplicity, users expect intuitive experience
- Talent Advantage: Strong UX research and design team
- **Score: 3** (can differentiate through design execution)

### Complete Pillar Scoring Example

| Strategic Pillar | Expected Impact | Certainty | Clarity of Levers | Uniqueness | Total | Selection Rationale |
| --- | --- | --- | --- | --- | --- | --- |
| **Discovery & Findability** | 5 | 4 | 4 | 3 | **16** | High user pain, strong evidence, clear solutions, design advantage |
| **Onboarding & Activation** | 4 | 4 | 4 | 3 | **15** | Business critical, proven problem, known solutions, UX strength |
| **Platform & Extensibility** | 3 | 3 | 4 | 4 | **14** | Enterprise growth, good tech advantage, clear roadmap |
| **Performance & Reliability** | 4 | 4 | 3 | 2 | **13** | User experience critical, but commoditized solutions |
| **Advanced Analytics** | 3 | 2 | 3 | 2 | **10** | Nice-to-have, uncertain demand, competitive disadvantage |

**Selected Pillars:** Top 3 scoring areas become strategic focus

### Scoring Workshop Facilitation Guide

#### Pre-Workshop Setup (15 minutes)
- Print scoring sheets for each participant
- Post opportunity areas on wall with descriptions
- Review scoring framework and examples
- Assign timekeeper and note-taker

#### Individual Scoring Round (45 minutes)
- Each participant scores independently
- No discussion during this phase
- Encourage participants to write reasoning notes
- Timekeeper gives 10-minute warnings

#### Score Sharing & Discussion (60 minutes)
- Go through each opportunity area systematically
- Each person shares their scores (no justification yet)
- Identify significant disagreements (2+ point differences)
- Focus discussion on areas with biggest gaps

#### Consensus Building (45 minutes)
- For each disagreement, ask highest and lowest scorers to explain reasoning
- Share evidence from preparation phase that supports different views
- Aim for consensus, but PM can make final call if needed
- Document final scores and key supporting evidence

#### Quality Check Questions
- Do our top-scoring pillars feel like the right strategic focus?
- Are we missing anything obvious from our top pillars?
- Do these pillars work well together as a portfolio?
- Can we defend these choices to skeptical stakeholders?

